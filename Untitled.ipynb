{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99144ae-518b-454a-844e-1726afdf8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa4ad55-5037-4d31-9072-4b38e22bbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_files(file_pattern='*.csv', encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Load multiple files and perform preprocessing\n",
    "    \n",
    "    Parameters:\n",
    "    file_pattern: str - Pattern to match files (e.g., '*.csv', 'data_*.csv')\n",
    "    encoding: str - File encoding (default 'utf-8', try 'latin-1' if errors)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all matching files\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(f\"No files found matching pattern: {file_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    # Load all files\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Try different separators if needed\n",
    "            df = pd.read_csv(file, encoding=encoding)\n",
    "            df['source_file'] = os.path.basename(file)  # Track source file\n",
    "            dataframes.append(df)\n",
    "            print(f\"‚úì Loaded {file}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading {file}: {e}\")\n",
    "            # Try with different encoding or separator\n",
    "            try:\n",
    "                df = pd.read_csv(file, encoding='latin-1', sep='\\t')\n",
    "                df['source_file'] = os.path.basename(file)\n",
    "                dataframes.append(df)\n",
    "                print(f\"‚úì Loaded {file} with latin-1 encoding: {len(df)} rows\")\n",
    "            except:\n",
    "                print(f\"‚úó Could not load {file}\")\n",
    "    \n",
    "    if len(dataframes) == 0:\n",
    "        print(\"No files could be loaded successfully\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nCombined dataset: {len(combined_df)} total rows\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14298ff3-f117-41d9-96ca-b87c383d34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING DATA PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = df.shape\n",
    "    print(f\"Original data shape: {original_shape}\")\n",
    "    \n",
    "    # 1. Handle column names - clean whitespace\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # 2. Display basic information\n",
    "    print(\"\\nColumn Information:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "    \n",
    "    # 3. Handle Date column\n",
    "    if 'date' in df.columns:\n",
    "        print(\"\\nüìÖ Processing Date column...\")\n",
    "        # Convert to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Extract additional date features\n",
    "        df['Year'] = df['date'].dt.year\n",
    "        df['Month'] = df['date'].dt.month\n",
    "        df['Day'] = df['date'].dt.day\n",
    "        df['Hour'] = df['date'].dt.hour\n",
    "        df['Weekday'] = df['date'].dt.day_name()\n",
    "        \n",
    "        print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # 4. Handle Location data\n",
    "    if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "        print(\"\\nüìç Processing Location data...\")\n",
    "        \n",
    "        # Convert to numeric\n",
    "        df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "        df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "        \n",
    "        # Check for invalid coordinates\n",
    "        invalid_coords = df[(df['Latitude'].isna()) | (df['Longitude'].isna())].shape[0]\n",
    "        if invalid_coords > 0:\n",
    "            print(f\"  Found {invalid_coords} rows with invalid coordinates\")\n",
    "        \n",
    "        # Basic coordinate validation\n",
    "        df.loc[~df['Latitude'].between(-90, 90), 'Latitude'] = np.nan\n",
    "        df.loc[~df['Longitude'].between(-180, 180), 'Longitude'] = np.nan\n",
    "    \n",
    "    # 5. Clean text fields\n",
    "    text_columns = ['Title', 'Author', 'Location_name', 'Tags']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\nüìù Cleaning {col} column...\")\n",
    "            # Remove extra whitespace\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "            # Replace 'nan' string with actual NaN\n",
    "            df[col] = df[col].replace(['nan', 'None', ''], np.nan)\n",
    "            \n",
    "            # Show unique count\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"  Unique values: {unique_count}\")\n",
    "    \n",
    "    # 6. Handle Post_id\n",
    "    if 'Post_id' in df.columns:\n",
    "        print(\"\\nüîë Processing Post_id...\")\n",
    "        df['Post_id'] = df['Post_id'].astype(str).str.strip()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df['Post_id'].duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"  ‚ö†Ô∏è Found {duplicates} duplicate Post_ids\")\n",
    "            # Remove duplicates, keeping first occurrence\n",
    "            df = df.drop_duplicates(subset=['Post_id'], keep='first')\n",
    "            print(f\"  Removed duplicates, new shape: {df.shape}\")\n",
    "    \n",
    "    # 7. Process Tags if present\n",
    "    if 'Tags' in df.columns:\n",
    "        print(\"\\nüè∑Ô∏è Processing Tags...\")\n",
    "        # Count tags per post\n",
    "        df['tag_count'] = df['Tags'].fillna('').str.split(',').str.len()\n",
    "        df.loc[df['Tags'].isna(), 'tag_count'] = 0\n",
    "        print(f\"  Average tags per post: {df['tag_count'].mean():.2f}\")\n",
    "    \n",
    "    # 8. Create additional features\n",
    "    print(\"\\n‚ú® Creating additional features...\")\n",
    "    \n",
    "    if 'Title' in df.columns:\n",
    "        # Title length\n",
    "        df['title_length'] = df['Title'].fillna('').str.len()\n",
    "        # Title word count\n",
    "        df['title_word_count'] = df['Title'].fillna('').str.split().str.len()\n",
    "    \n",
    "    # 9. Final cleaning\n",
    "    print(\"\\nüßπ Final cleaning...\")\n",
    "    \n",
    "    # Remove rows where all important columns are null\n",
    "    important_cols = ['Title', 'Author', 'Date']\n",
    "    important_cols = [col for col in important_cols if col in df.columns]\n",
    "    \n",
    "    before_drop = len(df)\n",
    "    df = df.dropna(subset=important_cols, how='all')\n",
    "    after_drop = len(df)\n",
    "    \n",
    "    if before_drop > after_drop:\n",
    "        print(f\"  Removed {before_drop - after_drop} rows with all null important columns\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREPROCESSING COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "    print(f\"Rows changed: {original_shape[0] - df.shape[0]}\")\n",
    "    print(f\"Columns added: {df.shape[1] - original_shape[1]}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2b37a5-30de-49a6-bce9-df1b8f0aefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics for the preprocessed data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Platform distribution\n",
    "    if 'Platform' in df.columns:\n",
    "        print(\"\\nüìä Platform Distribution:\")\n",
    "        print(df['Platform'].value_counts())\n",
    "    \n",
    "    # Location distribution\n",
    "    if 'Location_name' in df.columns:\n",
    "        print(\"\\nüìç Top 10 Locations:\")\n",
    "        print(df['Location_name'].value_counts().head(10))\n",
    "    \n",
    "    # Time analysis\n",
    "    if 'Hour' in df.columns:\n",
    "        print(\"\\n‚è∞ Posts by Hour:\")\n",
    "        hour_dist = df['Hour'].value_counts().sort_index()\n",
    "        for hour, count in hour_dist.head(5).items():\n",
    "            print(f\"  Hour {hour:02d}: {count} posts\")\n",
    "    \n",
    "    # Author activity\n",
    "    if 'Author' in df.columns:\n",
    "        print(\"\\nüë§ Top 10 Most Active Authors:\")\n",
    "        print(df['Author'].value_counts().head(10))\n",
    "    \n",
    "    # Numerical columns summary\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if numerical_cols:\n",
    "        print(\"\\nüìà Numerical Columns Summary:\")\n",
    "        print(df[numerical_cols].describe())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b20eb3-d896-4ddd-916f-b288e045e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data preprocessing pipeline...\n",
      "Found 5 files to process:\n",
      "  - germany_data/batch_41_20251108_163238.csv\n",
      "  - germany_data/batch_3_20251108_161040.csv\n",
      "  - germany_data/batch_2_20251108_155134.csv\n",
      "  - germany_data/batch_4_20251108_161040.csv\n",
      "  - germany_data/batch_1_20251108_153601.csv\n",
      "‚úì Loaded germany_data/batch_41_20251108_163238.csv: 453 rows\n",
      "‚úì Loaded germany_data/batch_3_20251108_161040.csv: 1339 rows\n",
      "‚úì Loaded germany_data/batch_2_20251108_155134.csv: 2070 rows\n",
      "‚úì Loaded germany_data/batch_4_20251108_161040.csv: 1000 rows\n",
      "‚úì Loaded germany_data/batch_1_20251108_153601.csv: 4000 rows\n",
      "\n",
      "Combined dataset: 8862 total rows\n",
      "\n",
      "==================================================\n",
      "STARTING DATA PREPROCESSING\n",
      "==================================================\n",
      "Original data shape: (8862, 12)\n",
      "\n",
      "Column Information:\n",
      "platform           object\n",
      "post_id            object\n",
      "title              object\n",
      "author             object\n",
      "date               object\n",
      "latitude          float64\n",
      "longitude         float64\n",
      "location_name      object\n",
      "tags               object\n",
      "url                object\n",
      "views_or_score      int64\n",
      "source_file        object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "No missing values found\n",
      "\n",
      "üìÖ Processing Date column...\n",
      "  Date range: 2020-02-02 04:03:22 to 2025-11-08 16:28:36\n",
      "\n",
      "‚ú® Creating additional features...\n",
      "\n",
      "üßπ Final cleaning...\n",
      "  Removed 8862 rows with all null important columns\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING COMPLETE\n",
      "==================================================\n",
      "Final data shape: (0, 17)\n",
      "Rows changed: 8862\n",
      "Columns added: 5\n",
      "\n",
      "==================================================\n",
      "SUMMARY STATISTICS\n",
      "==================================================\n",
      "\n",
      "‚è∞ Posts by Hour:\n",
      "\n",
      "üìà Numerical Columns Summary:\n",
      "       latitude  longitude  views_or_score  Year  Month  Day  Hour\n",
      "count       0.0        0.0             0.0   0.0    0.0  0.0   0.0\n",
      "mean        NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "std         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "min         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "25%         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "50%         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "75%         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "max         NaN        NaN             NaN   NaN    NaN  NaN   NaN\n",
      "\n",
      "‚úÖ Processed data saved to 'processed_combined_data.csv'\n",
      "\n",
      "üìã First 5 rows of processed data:\n",
      "Empty DataFrame\n",
      "Columns: [platform, post_id, title, author, date, latitude, longitude, location_name, tags, url, views_or_score, source_file, Year, Month, Day, Hour, Weekday]\n",
      "Index: []\n",
      "\n",
      "üìÑ Report saved to 'preprocessing_report.txt'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load files - adjust pattern as needed\n",
    "    # For CSV files: '*.csv'\n",
    "    # For specific files: 'data_*.csv' or ['file1.csv', 'file2.csv']\n",
    "    \n",
    "    print(\"üöÄ Starting data preprocessing pipeline...\")\n",
    "    \n",
    "    # Load all CSV files in current directory\n",
    "    combined_data = load_and_preprocess_files('germany_data/*.csv')\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        # Preprocess the data\n",
    "        processed_data = preprocess_data(combined_data)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        final_data = generate_summary_statistics(processed_data)\n",
    "        \n",
    "        # Save the processed data\n",
    "        output_filename = 'processed_combined_data.csv'\n",
    "        final_data.to_csv(output_filename, index=False)\n",
    "        print(f\"\\n‚úÖ Processed data saved to '{output_filename}'\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\nüìã First 5 rows of processed data:\")\n",
    "        print(final_data.head())\n",
    "        \n",
    "        # Optional: Save a summary report\n",
    "        with open('preprocessing_report.txt', 'w') as f:\n",
    "            f.write(\"Data Preprocessing Report\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"Total rows processed: {len(final_data)}\\n\")\n",
    "            f.write(f\"Total columns: {len(final_data.columns)}\\n\")\n",
    "            f.write(f\"Date range: {final_data['date'].min()} to {final_data['date'].max()}\\n\")\n",
    "            f.write(f\"\\nColumns: {', '.join(final_data.columns)}\\n\")\n",
    "        \n",
    "        print(\"\\nüìÑ Report saved to 'preprocessing_report.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43f38d-bc85-41f7-8b75-8e3eee02fbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c645cd34-b327-4783-aa4e-92ad62df3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20439aac-40ed-47bb-a1f3-715bf6265ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61306bc0-0bd7-4a3e-875f-90df726dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAStTrafficScraper:\n",
    "    \"\"\"Scraper for BASt traffic data from German highways and federal roads\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"bast_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the BASt traffic scraper\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save scraped data\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Base URLs for BASt data\n",
    "        self.base_url = \"https://www.bast.de\"\n",
    "        self.data_url = f\"{self.base_url}/DE/Publikationen/Daten/Verkehrstechnik\"\n",
    "        \n",
    "        # Alternative API endpoints (based on research)\n",
    "        self.autobahn_api_url = \"https://verkehr.autobahn.de/o/autobahn/\"\n",
    "        \n",
    "        # Headers for requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        # Station metadata storage\n",
    "        self.stations_metadata = {}\n",
    "        self.traffic_data = []\n",
    "        \n",
    "    def download_monthly_data(self, year: int, month: int) -> Optional[bytes]:\n",
    "        \"\"\"\n",
    "        Download monthly traffic data ZIP file from BASt\n",
    "        \n",
    "        Args:\n",
    "            year: Year of data\n",
    "            month: Month of data\n",
    "            \n",
    "        Returns:\n",
    "            Bytes content of ZIP file or None if download fails\n",
    "        \"\"\"\n",
    "        # Format: BASt provides monthly ZIP files\n",
    "        month_str = f\"{month:02d}\"\n",
    "        year_str = str(year)\n",
    "        \n",
    "        # Try different URL patterns (BASt changes structure occasionally)\n",
    "        url_patterns = [\n",
    "            f\"{self.data_url}/DZ-Richtung_{year_str}_{month_str}.zip\",\n",
    "            f\"{self.data_url}/DZ_{year_str}_{month_str}.zip\",\n",
    "            f\"{self.data_url}/Rohdaten_{year_str}_{month_str}.zip\"\n",
    "        ]\n",
    "        \n",
    "        for url in url_patterns:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to download from: {url}\")\n",
    "                response = requests.get(url, headers=self.headers, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    logger.info(f\"Successfully downloaded data for {year}-{month}\")\n",
    "                    return response.content\n",
    "            except requests.RequestException as e:\n",
    "                logger.warning(f\"Failed to download from {url}: {e}\")\n",
    "                \n",
    "        logger.error(f\"Could not download data for {year}-{month}\")\n",
    "        return None\n",
    "    \n",
    "    def extract_station_metadata(self, zip_content: bytes) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract station metadata including coordinates from ZIP file\n",
    "        \n",
    "        Args:\n",
    "            zip_content: Content of ZIP file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with station metadata\n",
    "        \"\"\"\n",
    "        metadata_list = []\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(io.BytesIO(zip_content)) as zf:\n",
    "                # Look for metadata CSV files\n",
    "                for filename in zf.namelist():\n",
    "                    if 'metadata' in filename.lower() or 'stammdaten' in filename.lower():\n",
    "                        with zf.open(filename) as f:\n",
    "                            # Read CSV with German encoding\n",
    "                            df = pd.read_csv(f, encoding='iso-8859-1', sep=';', \n",
    "                                          decimal=',', on_bad_lines='skip')\n",
    "                            \n",
    "                            # Map common column names\n",
    "                            column_mapping = {\n",
    "                                'Zst': 'station_id',\n",
    "                                'ZST': 'station_id',\n",
    "                                'Name': 'station_name',\n",
    "                                'Straße': 'road',\n",
    "                                'Strasse': 'road',\n",
    "                                'Nr': 'road_number',\n",
    "                                'Bundesland': 'state',\n",
    "                                'Land': 'state',\n",
    "                                'Breite': 'latitude',\n",
    "                                'Lat': 'latitude',\n",
    "                                'Länge': 'longitude',\n",
    "                                'Lng': 'longitude',\n",
    "                                'Lon': 'longitude',\n",
    "                                'X': 'longitude',\n",
    "                                'Y': 'latitude',\n",
    "                                'Rechtswert': 'x_coord',\n",
    "                                'Hochwert': 'y_coord'\n",
    "                            }\n",
    "                            \n",
    "                            df.rename(columns=column_mapping, inplace=True)\n",
    "                            metadata_list.append(df)\n",
    "                            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting metadata: {e}\")\n",
    "            \n",
    "        if metadata_list:\n",
    "            return pd.concat(metadata_list, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def get_station_coordinates(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get station coordinates from BASt interactive map or API\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with station IDs and coordinates\n",
    "        \"\"\"\n",
    "        stations = []\n",
    "        \n",
    "        # Try to get station list from API\n",
    "        try:\n",
    "            # This is a simplified example - actual implementation would need\n",
    "            # to parse the BASt website or use their data endpoints\n",
    "            api_url = f\"{self.base_url}/DE/Verkehrstechnik/Fachthemen/v2-verkehrszaehlung/Daten/2024_1/Jawe2024.json\"\n",
    "            response = requests.get(api_url, headers=self.headers, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for station in data.get('stations', []):\n",
    "                    stations.append({\n",
    "                        'station_id': station.get('id'),\n",
    "                        'station_name': station.get('name'),\n",
    "                        'road': station.get('road'),\n",
    "                        'road_number': station.get('road_number'),\n",
    "                        'latitude': station.get('lat'),\n",
    "                        'longitude': station.get('lon'),\n",
    "                        'state': station.get('state')\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not fetch from API: {e}\")\n",
    "            \n",
    "        # Fallback: Use predefined major counting stations with known coordinates\n",
    "        # These are example stations - in production, you'd have a complete list\n",
    "        if not stations:\n",
    "            stations = self._get_example_stations()\n",
    "            \n",
    "        return pd.DataFrame(stations)\n",
    "    \n",
    "    def _get_example_stations(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get example counting stations with coordinates\n",
    "        This is a sample - actual implementation would have complete data\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'station_id': 'A1-001',\n",
    "                'station_name': 'Köln-West',\n",
    "                'road': 'A1',\n",
    "                'road_number': '1',\n",
    "                'latitude': 50.9375,\n",
    "                'longitude': 6.9603,\n",
    "                'state': 'Nordrhein-Westfalen'\n",
    "            },\n",
    "            {\n",
    "                'station_id': 'A2-001',\n",
    "                'station_name': 'Berlin-West',\n",
    "                'road': 'A2',\n",
    "                'road_number': '2',\n",
    "                'latitude': 52.5200,\n",
    "                'longitude': 13.4050,\n",
    "                'state': 'Berlin'\n",
    "            },\n",
    "            {\n",
    "                'station_id': 'A3-001',\n",
    "                'station_name': 'Frankfurt-Main',\n",
    "                'road': 'A3',\n",
    "                'road_number': '3',\n",
    "                'latitude': 50.1109,\n",
    "                'longitude': 8.6821,\n",
    "                'state': 'Hessen'\n",
    "            },\n",
    "            {\n",
    "                'station_id': 'A7-001',\n",
    "                'station_name': 'Hamburg-Süd',\n",
    "                'road': 'A7',\n",
    "                'road_number': '7',\n",
    "                'latitude': 53.5511,\n",
    "                'longitude': 9.9937,\n",
    "                'state': 'Hamburg'\n",
    "            },\n",
    "            {\n",
    "                'station_id': 'A9-001',\n",
    "                'station_name': 'München-Nord',\n",
    "                'road': 'A9',\n",
    "                'road_number': '9',\n",
    "                'latitude': 48.1351,\n",
    "                'longitude': 11.5820,\n",
    "                'state': 'Bayern'\n",
    "            },\n",
    "            {\n",
    "                'station_id': 'B1-001',\n",
    "                'station_name': 'Magdeburg',\n",
    "                'road': 'B1',\n",
    "                'road_number': '1',\n",
    "                'latitude': 52.1205,\n",
    "                'longitude': 11.6276,\n",
    "                'state': 'Sachsen-Anhalt'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def fetch_traffic_counts(self, station_id: str, date: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch traffic count data for a specific station and date\n",
    "        \n",
    "        Args:\n",
    "            station_id: Station identifier\n",
    "            date: Date in YYYY-MM-DD format\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with traffic count data\n",
    "        \"\"\"\n",
    "        traffic_data = {\n",
    "            'station_id': station_id,\n",
    "            'date': date,\n",
    "            'total_vehicles': 0,\n",
    "            'cars': 0,\n",
    "            'trucks': 0,\n",
    "            'motorcycles': 0,\n",
    "            'buses': 0,\n",
    "            'hourly_counts': []\n",
    "        }\n",
    "        \n",
    "        # This would be replaced with actual API calls or file parsing\n",
    "        # For demonstration, generating sample data\n",
    "        import random\n",
    "        \n",
    "        # Generate hourly counts (24 hours)\n",
    "        for hour in range(24):\n",
    "            count = {\n",
    "                'hour': hour,\n",
    "                'vehicles': random.randint(100, 2000),\n",
    "                'speed_avg': random.randint(80, 130)\n",
    "            }\n",
    "            traffic_data['hourly_counts'].append(count)\n",
    "            traffic_data['total_vehicles'] += count['vehicles']\n",
    "        \n",
    "        # Distribute vehicle types (approximate distribution)\n",
    "        traffic_data['cars'] = int(traffic_data['total_vehicles'] * 0.75)\n",
    "        traffic_data['trucks'] = int(traffic_data['total_vehicles'] * 0.20)\n",
    "        traffic_data['buses'] = int(traffic_data['total_vehicles'] * 0.03)\n",
    "        traffic_data['motorcycles'] = int(traffic_data['total_vehicles'] * 0.02)\n",
    "        \n",
    "        return traffic_data\n",
    "    \n",
    "    def scrape_autobahn_api(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape data from the Autobahn GmbH API\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with traffic and construction data\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        try:\n",
    "            # Get list of autobahns\n",
    "            response = requests.get(self.autobahn_api_url, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                autobahns = response.json().get('roads', [])\n",
    "                \n",
    "                for autobahn in autobahns[:5]:  # Limit for demonstration\n",
    "                    # Get details for each autobahn\n",
    "                    detail_url = f\"{self.autobahn_api_url}{autobahn}\"\n",
    "                    detail_response = requests.get(detail_url, headers=self.headers)\n",
    "                    \n",
    "                    if detail_response.status_code == 200:\n",
    "                        details = detail_response.json()\n",
    "                        \n",
    "                        # Extract relevant information\n",
    "                        for item in details.get('items', []):\n",
    "                            data.append({\n",
    "                                'road': autobahn,\n",
    "                                'type': item.get('type'),\n",
    "                                'title': item.get('title'),\n",
    "                                'subtitle': item.get('subtitle'),\n",
    "                                'coordinate': item.get('coordinate'),\n",
    "                                'extent': item.get('extent'),\n",
    "                                'timestamp': datetime.now().isoformat()\n",
    "                            })\n",
    "                            \n",
    "                    time.sleep(0.5)  # Be respectful with API calls\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Autobahn API: {e}\")\n",
    "            \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"Save DataFrame to CSV file\"\"\"\n",
    "        filepath = self.output_dir / filename\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Saved data to {filepath}\")\n",
    "    \n",
    "    def save_to_json(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"Save DataFrame to JSON file\"\"\"\n",
    "        filepath = self.output_dir / filename\n",
    "        df.to_json(filepath, orient='records', indent=2, force_ascii=False)\n",
    "        logger.info(f\"Saved data to {filepath}\")\n",
    "    \n",
    "    def run(self, year: int = 2024, month: int = 1):\n",
    "        \"\"\"\n",
    "        Main execution method\n",
    "        \n",
    "        Args:\n",
    "            year: Year to scrape data for\n",
    "            month: Month to scrape data for\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting BASt traffic data scraper...\")\n",
    "        \n",
    "        # 1. Get station coordinates\n",
    "        logger.info(\"Fetching station coordinates...\")\n",
    "        stations_df = self.get_station_coordinates()\n",
    "        \n",
    "        if not stations_df.empty:\n",
    "            self.save_to_csv(stations_df, 'stations_with_coordinates.csv')\n",
    "            self.save_to_json(stations_df, 'stations_with_coordinates.json')\n",
    "            \n",
    "            # 2. Fetch traffic data for each station\n",
    "            logger.info(\"Fetching traffic count data...\")\n",
    "            traffic_records = []\n",
    "            \n",
    "            for _, station in stations_df.iterrows():\n",
    "                # Get traffic data for the station\n",
    "                date_str = f\"{year}-{month:02d}-01\"\n",
    "                traffic_data = self.fetch_traffic_counts(\n",
    "                    station['station_id'], \n",
    "                    date_str\n",
    "                )\n",
    "                \n",
    "                # Combine with location data\n",
    "                traffic_record = {\n",
    "                    'station_id': station['station_id'],\n",
    "                    'station_name': station['station_name'],\n",
    "                    'road': station['road'],\n",
    "                    'latitude': station['latitude'],\n",
    "                    'longitude': station['longitude'],\n",
    "                    'state': station['state'],\n",
    "                    'date': date_str,\n",
    "                    'total_vehicles': traffic_data['total_vehicles'],\n",
    "                    'cars': traffic_data['cars'],\n",
    "                    'trucks': traffic_data['trucks'],\n",
    "                    'buses': traffic_data['buses'],\n",
    "                    'motorcycles': traffic_data['motorcycles']\n",
    "                }\n",
    "                traffic_records.append(traffic_record)\n",
    "                \n",
    "            traffic_df = pd.DataFrame(traffic_records)\n",
    "            self.save_to_csv(traffic_df, 'traffic_data_with_locations.csv')\n",
    "            self.save_to_json(traffic_df, 'traffic_data_with_locations.json')\n",
    "            \n",
    "        # 3. Try to scrape Autobahn API data\n",
    "        logger.info(\"Fetching data from Autobahn API...\")\n",
    "        autobahn_df = self.scrape_autobahn_api()\n",
    "        if not autobahn_df.empty:\n",
    "            self.save_to_csv(autobahn_df, 'autobahn_api_data.csv')\n",
    "            \n",
    "        logger.info(\"Scraping completed successfully!\")\n",
    "        \n",
    "        # Create summary\n",
    "        self._create_summary(stations_df, traffic_df if 'traffic_df' in locals() else pd.DataFrame())\n",
    "    \n",
    "    def _create_summary(self, stations_df: pd.DataFrame, traffic_df: pd.DataFrame):\n",
    "        \"\"\"Create a summary report of scraped data\"\"\"\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_stations': len(stations_df),\n",
    "            'stations_with_coordinates': len(stations_df[stations_df['latitude'].notna()]),\n",
    "            'total_traffic_records': len(traffic_df),\n",
    "            'states_covered': stations_df['state'].nunique() if 'state' in stations_df.columns else 0,\n",
    "            'roads_covered': stations_df['road'].nunique() if 'road' in stations_df.columns else 0\n",
    "        }\n",
    "        \n",
    "        if not traffic_df.empty:\n",
    "            summary['total_vehicles_counted'] = traffic_df['total_vehicles'].sum()\n",
    "            summary['average_vehicles_per_station'] = traffic_df['total_vehicles'].mean()\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = self.output_dir / 'scraping_summary.json'\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Summary saved to {summary_path}\")\n",
    "        logger.info(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9390469e-af7a-4faf-b31b-c17973735773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 18:17:38,783 - INFO - Starting BASt traffic data scraper...\n",
      "2025-11-08 18:17:38,785 - INFO - Fetching station coordinates...\n",
      "2025-11-08 18:17:41,220 - INFO - Saved data to bast_traffic_data/stations_with_coordinates.csv\n",
      "2025-11-08 18:17:41,225 - INFO - Saved data to bast_traffic_data/stations_with_coordinates.json\n",
      "2025-11-08 18:17:41,226 - INFO - Fetching traffic count data...\n",
      "2025-11-08 18:17:41,229 - INFO - Saved data to bast_traffic_data/traffic_data_with_locations.csv\n",
      "2025-11-08 18:17:41,230 - INFO - Saved data to bast_traffic_data/traffic_data_with_locations.json\n",
      "2025-11-08 18:17:41,230 - INFO - Fetching data from Autobahn API...\n",
      "2025-11-08 18:17:48,347 - INFO - Scraping completed successfully!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Run scraper for current year and month\u001b[39;00m\n\u001b[32m      7\u001b[39m current_date = datetime.now()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m.\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Example: Scrape specific period\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# scraper.run(year=2024, month=10)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Scraping Complete ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 376\u001b[39m, in \u001b[36mBAStTrafficScraper.run\u001b[39m\u001b[34m(self, year, month)\u001b[39m\n\u001b[32m    373\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mScraping completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# Create summary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstations_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraffic_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraffic_df\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 396\u001b[39m, in \u001b[36mBAStTrafficScraper._create_summary\u001b[39m\u001b[34m(self, stations_df, traffic_df)\u001b[39m\n\u001b[32m    394\u001b[39m summary_path = \u001b[38;5;28mself\u001b[39m.output_dir / \u001b[33m'\u001b[39m\u001b[33mscraping_summary.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(summary_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    399\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Create scraper instance\n",
    "    scraper = BAStTrafficScraper(output_dir=\"bast_traffic_data\")\n",
    "    \n",
    "    # Run scraper for current year and month\n",
    "    current_date = datetime.now()\n",
    "    scraper.run(year=current_date.year, month=current_date.month)\n",
    "    \n",
    "    # Example: Scrape specific period\n",
    "    # scraper.run(year=2024, month=10)\n",
    "    \n",
    "    print(\"\\n=== Scraping Complete ===\")\n",
    "    print(f\"Data saved to: {scraper.output_dir}\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    for file in scraper.output_dir.glob(\"*\"):\n",
    "        print(f\"  - {file.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe345c-3680-40ab-8e02-235681a3fe33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
